\documentclass{article}
\usepackage{enumerate}
\usepackage{amsmath}

\begin{document}

\title{\huge \textbf{Stat 207 HW6} \\}
\author{\large Cheng Luo 912466499 \\ \large Fan Wu 912538518}
\maketitle

\newpage
\mbox{}
\newpage

\section{14.9}

\begin{enumerate}[(a)]

\item

<<>>=
  dat = read.table("CH14PR09.txt")
  names(dat) = c("Y", "X")
  logit = glm(Y ~ X, data = dat, family = "binomial")
  summary(logit)
  b0 = coef(logit)[1]; b0
  b1 = coef(logit)[2]; b1
@

\qquad From the summary, the maximum likelihood estimates of $b_0 = -10.308925$, $b_1 = 0.018920$, $$\hat{\pi} = \frac{exp(b_0+b_1 X)}{1+exp(b_0+b_1 X)}=\frac{exp(-10.308925+0.018920 X)}{1+exp(-10.308925+0.018920 X)}$$

\item

<<>>=
  plot(dat$X, dat$Y)
  points(dat$X, fitted(logit), type = 'l', lty = 1)
  points(dat$X, lowess(dat$X, dat$Y)$y, type = 'l', lty = 2)
  legend('right', legend = c('fitted', 'lowess'), 
       lty = 1:2)
@

\qquad The fitted logistic response function appears to be well.

\item

<<>>=
  exp(0.018920)
@

\qquad $exp(\beta_1)=1.0191$, so that the odds of employee's ability increased by 1.91\% with each additional employee's emotional stability.

\item

<<>>=
  newdat = data.frame(X = 550)
  predict(logit, newdata = newdat, type = "response")
@

\qquad The estimated probability that employees with an emotional stability test score of 550 will be able to perform in a task group is 0.5242263 .

\end{enumerate}

\section{Problem 5}

\begin{enumerate}[(a)]

\item

<<>>=
  dat = read.table("apartment.txt", header = TRUE)
  require("pls")

  dat.stan = dat
  for(j in 1:ncol(dat))
    dat.stan[,j] = (dat[,j] - mean(dat[,j]))/sd(dat[,j])
  n = nrow(dat); n

  fit = plsr(Y ~0 + ., data = dat.stan, 5, validation = "CV")
  summary(fit)
  scores(fit)
  loadings(fit)[, 1:3]

  k = 1:6
  r.sq = c(92.14 ,   96.53  ,  97.92  ,  98.01 ,   98.05)/100
  r.adj = 1 - (n-1)/(n-k-1)*(1-r.sq); r.adj

  plot(fit, plottype = "scores", comps = 1:3)
@

\item

<<>>=
  r.sq = c(0, r.sq); r.sq
  f.k = sapply(2:length(r.sq), function(k) 
    (n-k-1)*(r.sq[k] - r.sq[k-1])/(1-r.sq[k])); f.k
  qf(1-0.05, 1, 1:5)
@

\qquad As we can see from above, we might decide the number of components to keep is 3.

\item

<<>>=
  model = plsr(Y ~ 0 + ., 
               3, data = dat.stan, validation = 'CV')
  coef(model)
  plot(fitted(model)[,,3], dat.stan[,1])
  plot(fitted(model)[,,3], resid(model)[,,3])
  hist(resid(model)[,,3])
@

\qquad The final model is $$Y^* = -0.11356364 X_1^* + 0.34543343 X_2^* -0.02384503 X_3^* + 0.05143543 X_4^* +0.67482746 X_5^* $$
\qquad The observed against the fitted values plots shows it fits well, and residuals against the fitted values and the histogram of the residuals plots show it has no sign for unequal variance.

\end{enumerate}

\section{Problem 6}

\begin{enumerate}[(a)]

\item

<<>>=
  require(glmnet)
  x = as.matrix(dat.stan[, -1])
  model = cv.glmnet(x, dat.stan[, 1], intercept = FALSE)
  plot(model)
  model$lambda.min
@

\item

<<>>=
  coef(model)
  
  fitted. = predict(model, newx = x)
  plot(fitted., dat.stan[,1])
  
  res = dat.stan[,1] - fitted.
  plot(fitted., res)
  hist(res)
@

\qquad The final model is $$Y^* =  -0.0610260558 X_1^* + 0.2646220373 X_2^* + 0.0002055733 X_3^* + 0.0237525845 X_4^* +0.6758144083 X_5^* $$
\qquad The observed against the fitted values plots shows it fits well, and residuals against the fitted values and the histogram of the residuals plots show it has no sign for unequal variance.

\end{enumerate}

\section{Problem 7}

\section{Problem 8}

\begin{enumerate}[(a)]

\item

<<>>=
  lambda = c(19, 3, 1, .7, .3)
  e.beta = c(.8, .3, .2, .2, .1)
  sig.sq = 2.5
  
  k.seq = seq(0, 1000, 1)
  
  d.foo = function(k, sig.sq, lambda, e.beta) 
  {
    sig.sq * sum( lambda / (k+lambda)^2 ) + 
      k^2 * sum( e.beta^2 / (k+lambda)^2 )
  }
  
  d.eval = sapply(k.seq, function(k) 
    d.foo(k, sig.sq, lambda, e.beta))
  plot(k.seq, d.eval)
  
  d.opt = optimize(d.foo, c(0, 100), sig.sq, lambda, e.beta); d.opt
@

\item

<<>>=
lambda = c(19, 3, 1, .7, .3)
e.beta = c(.8, .3, .2, .2, .1)
sig.sq = 2.5

k.seq = seq(0, 1000, 1)

l.foo = function(k, sig.sq, lambda, e.beta) 
{
  sig.sq * sum( lambda^2 / (k+lambda)^2 ) + 
    k^2 * sum( e.beta^2*lambda / (k+lambda)^2 )
}

l.eval = sapply(k.seq, function(k) 
  l.foo(k, sig.sq, lambda, e.beta))
plot(k.seq, l.eval)

l.opt = optimize(l.foo, c(0, 100), sig.sq, lambda, e.beta); l.opt
@

\item

<<>>=
  d_0 = d.foo(0, sig.sq, lambda, e.beta); d_0
  d.opt   

  l_0 = l.foo(0, sig.sq, lambda, e.beta); l_0
  l.opt 
@

\qquad For D(k), D(k.opt)$<$D(0), it's possible to improve over the ordinary least squares method using ridge regression.
For L(k), L(k.opt)$<$L(0), which also means it's possible to improve over the ordinary least squares method using ridge regression.



\end{enumerate}

\end{document}